{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23e8447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-documentintelligence in c:\\users\\anaritak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: azure-ai-textanalytics in c:\\users\\anaritak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (5.3.0)\n",
      "Requirement already satisfied: azure-core in c:\\users\\anaritak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.35.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\anaritak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\anaritak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from azure-ai-documentintelligence) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\anaritak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from azure-ai-documentintelligence) (4.14.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\anaritak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from azure-ai-textanalytics) (1.1.28)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\anaritak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from azure-core) (2.32.4)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\anaritak\\appdata\\roaming\\python\\python313\\site-packages (from azure-core) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\anaritak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.21.0->azure-core) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anaritak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.21.0->azure-core) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anaritak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.21.0->azure-core) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anaritak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.21.0->azure-core) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# REQUERIMENTS\n",
    "# =============================================================================\n",
    "\n",
    "!pip install azure-ai-documentintelligence azure-ai-textanalytics azure-core Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23e8447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION AND IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# Azure imports\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Other imports\n",
    "import fitz\n",
    "from dotenv import load_dotenv\n",
    "try:\n",
    "    from PIL import Image, ImageDraw\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'Pillow'])\n",
    "    from PIL import Image, ImageDraw\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('k.env')\n",
    "\n",
    "# Azure service configuration\n",
    "AZURE_DOCINT_ENDPOINT = os.getenv('AZURE_DOCINT_ENDPOINT', 'https://SEU-ENDPOINT.cognitiveservices.azure.com/')\n",
    "AZURE_DOCINT_KEY = os.getenv('AZURE_DOCINT_KEY', 'COLOQUE_SUA_CHAVE')\n",
    "AZURE_LANGUAGE_ENDPOINT = os.getenv('AZURE_LANGUAGE_ENDPOINT', AZURE_DOCINT_ENDPOINT)\n",
    "AZURE_LANGUAGE_KEY = os.getenv('AZURE_LANGUAGE_KEY', AZURE_DOCINT_KEY)\n",
    "\n",
    "if 'COLOQUE_SUA_CHAVE' in (AZURE_DOCINT_KEY, AZURE_LANGUAGE_KEY):\n",
    "    print('ATEN√á√ÉO: configure suas chaves nas vari√°veis de ambiente.')\n",
    "\n",
    "# Directory configuration\n",
    "DATA_DIR = os.getenv('PDF_INPUT_DIR', 'data')\n",
    "OUT_DIR = os.getenv('PDF_OUTPUT_DIR', 'anonimized')\n",
    "\n",
    "DATA_PATH = Path(DATA_DIR)\n",
    "OUT_PATH = Path(OUT_DIR)\n",
    "OUT_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Feature flags\n",
    "VERBOSE_OCR = False\n",
    "REDACT_BARCODES = os.getenv('REDACT_BARCODES', '1') == '1'\n",
    "\n",
    "# ALLOWLIST CONFIGURATION\n",
    "ALLOWED_ORGANIZATIONS = [\n",
    "    'Contoso',\n",
    "    'CONTOSO LTDA',\n",
    "    'MICROSOFT'\n",
    "]\n",
    "\n",
    "ALLOWED_PERSONS = [\n",
    "    'Solicitant'\n",
    "]\n",
    "\n",
    "# Convert to lowercase for case-insensitive matching\n",
    "ALLOWED_ORGANIZATIONS_LOWER = [org.lower() for org in ALLOWED_ORGANIZATIONS]\n",
    "ALLOWED_PERSONS_LOWER = [person.lower() for person in ALLOWED_PERSONS]\n",
    "\n",
    "# DATE FILTERING CONFIGURATION\n",
    "PROTECTED_DATE_CATEGORIES = [\n",
    "    'DateTime',\n",
    "    'Date', \n",
    "    'Time',\n",
    "    'DateRange',\n",
    "    'TimeRange'\n",
    "]\n",
    "\n",
    "REDACTED_DATE_CATEGORIES = [\n",
    "    'DateOfBirth'\n",
    "]\n",
    "\n",
    "# URL FILTERING CONFIGURATION\n",
    "REDACT_URL_PATTERNS = [\n",
    "    r'https://learn.microsoft.com/'\n",
    "]\n",
    "\n",
    "# Compile regex patterns for efficiency\n",
    "REDACT_URL_REGEX = [re.compile(pattern, re.IGNORECASE) for pattern in REDACT_URL_PATTERNS]\n",
    "\n",
    "# Text processing thresholds\n",
    "TEXT_SIZE_THRESHOLDS = {\n",
    "    'single_call_max': 4500,\n",
    "    'chunk_size': 4000,\n",
    "    'mini_chunk_size': 2000,\n",
    "    'emergency_chunk_size': 3000\n",
    "}\n",
    "\n",
    "# Initialize execution control\n",
    "if 'EXECUTION_CONTROL' not in globals():\n",
    "    EXECUTION_CONTROL = {\n",
    "        'pipeline_executed': False,\n",
    "        'inspection_executed': False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c05c60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA CLASSES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class WordBox:\n",
    "    page: int\n",
    "    text: str\n",
    "    offset: int          \n",
    "    length: int\n",
    "    bbox: Tuple[float, float, float, float]  \n",
    "\n",
    "@dataclass\n",
    "class EntityPII:\n",
    "    category: str\n",
    "    offset: int          \n",
    "    length: int\n",
    "    text: str\n",
    "    page: int = -1     \n",
    "\n",
    "@dataclass\n",
    "class Redaction:\n",
    "    page: int\n",
    "    rect: Tuple[float, float, float, float]\n",
    "    label: str\n",
    "\n",
    "@dataclass\n",
    "class TextChunk:\n",
    "    \"\"\"Represents a text chunk with offset mapping\"\"\"\n",
    "    text: str\n",
    "    start_offset: int\n",
    "    end_offset: int\n",
    "    chunk_id: str\n",
    "\n",
    "@dataclass\n",
    "class ChunkEntity:\n",
    "    \"\"\"Entity found in a chunk with local offsets\"\"\"\n",
    "    category: str\n",
    "    chunk_offset: int\n",
    "    length: int\n",
    "    text: str\n",
    "    global_offset: int\n",
    "    chunk_id: str\n",
    "\n",
    "@dataclass\n",
    "class PageResult:\n",
    "    \"\"\"Results from processing a single page\"\"\"\n",
    "    page_num: int\n",
    "    entities: List[EntityPII]\n",
    "    redactions: List[Redaction] \n",
    "    barcodes: List[Dict[str, Any]]\n",
    "    success: bool\n",
    "    error_message: str = \"\"\n",
    "    word_count: int = 0\n",
    "    redacted_image_path: Path = None\n",
    "    text_analysis: Dict[str, Any] = None  \n",
    "    allowed_entities: List[EntityPII] = None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e59fa136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def merge_rects(rects: List[Tuple[float,float,float,float]], gap: float = 2.0) -> List[Tuple[float,float,float,float]]:\n",
    "    \"\"\"Merge rectangles that are very close to each other\"\"\"\n",
    "    if not rects:\n",
    "        return []\n",
    "    rects_sorted = sorted(rects, key=lambda r: (round(r[1]/10), r[0]))\n",
    "    merged: List[Tuple[float,float,float,float]] = []\n",
    "    cur = list(rects_sorted[0])\n",
    "    for r in rects_sorted[1:]:\n",
    "        same_line = abs(r[1]-cur[1]) < 5 and abs(r[3]-cur[3]) < 10\n",
    "        touches = r[0] - cur[2] <= gap\n",
    "        if same_line and touches:\n",
    "            cur[2] = max(cur[2], r[2])\n",
    "            cur[3] = max(cur[3], r[3])\n",
    "            cur[0] = min(cur[0], r[0])\n",
    "            cur[1] = min(cur[1], r[1])\n",
    "        else:\n",
    "            merged.append(tuple(cur))\n",
    "            cur = list(r)\n",
    "    merged.append(tuple(cur))\n",
    "    return merged\n",
    "\n",
    "def vlog(*a):\n",
    "    \"\"\"Verbose logging for OCR operations\"\"\"\n",
    "    if VERBOSE_OCR:\n",
    "        print('[OCR]', *a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "965cc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AZURE CLIENT INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "_DOCINT_CLIENT = None\n",
    "_PII_CLIENT = None\n",
    "\n",
    "def get_docint_client():\n",
    "    \"\"\"Get or create Document Intelligence client\"\"\"\n",
    "    global _DOCINT_CLIENT\n",
    "    if _DOCINT_CLIENT is None:\n",
    "        _DOCINT_CLIENT = DocumentIntelligenceClient(\n",
    "            endpoint=AZURE_DOCINT_ENDPOINT,\n",
    "            credential=AzureKeyCredential(AZURE_DOCINT_KEY)\n",
    "        )\n",
    "    return _DOCINT_CLIENT\n",
    "\n",
    "def get_pii_client():\n",
    "    \"\"\"Get or create Text Analytics client\"\"\"\n",
    "    global _PII_CLIENT\n",
    "    if _PII_CLIENT is None:\n",
    "        _PII_CLIENT = TextAnalyticsClient(\n",
    "            endpoint=AZURE_LANGUAGE_ENDPOINT,\n",
    "            credential=AzureKeyCredential(AZURE_LANGUAGE_KEY)\n",
    "        )\n",
    "    return _PII_CLIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96d8ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEXT CHUNKING AND ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_text_size(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze text size and determine processing strategy\"\"\"\n",
    "    text_length = len(text)\n",
    "    \n",
    "    if text_length <= TEXT_SIZE_THRESHOLDS['single_call_max']:\n",
    "        strategy = 'single_call'\n",
    "        chunks_needed = 1\n",
    "    elif text_length <= TEXT_SIZE_THRESHOLDS['chunk_size'] * 10:\n",
    "        strategy = 'standard_chunks'\n",
    "        chunks_needed = (text_length // TEXT_SIZE_THRESHOLDS['chunk_size']) + 1\n",
    "    else:\n",
    "        strategy = 'mini_chunks'\n",
    "        chunks_needed = (text_length // TEXT_SIZE_THRESHOLDS['mini_chunk_size']) + 1\n",
    "    \n",
    "    return {\n",
    "        'length': text_length,\n",
    "        'strategy': strategy,\n",
    "        'chunks_needed': chunks_needed,\n",
    "        'threshold_used': TEXT_SIZE_THRESHOLDS['single_call_max']\n",
    "    }\n",
    "\n",
    "def create_text_chunks(text: str, chunk_size: int, overlap: int = 100) -> List[TextChunk]:\n",
    "    \"\"\"Create overlapping text chunks with offset tracking\"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [TextChunk(\n",
    "            text=text,\n",
    "            start_offset=0,\n",
    "            end_offset=len(text),\n",
    "            chunk_id=\"chunk_0\"\n",
    "        )]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_num = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        \n",
    "        if end < len(text):\n",
    "            break_point = end\n",
    "            for i in range(end - 50, end):\n",
    "                if i > start and text[i] in ' \\n\\t.,;:!?':\n",
    "                    break_point = i + 1\n",
    "                    break\n",
    "            end = break_point\n",
    "        \n",
    "        chunk_text = text[start:end]\n",
    "        chunks.append(TextChunk(\n",
    "            text=chunk_text,\n",
    "            start_offset=start,\n",
    "            end_offset=end,\n",
    "            chunk_id=f\"chunk_{chunk_num}\"\n",
    "        ))\n",
    "        \n",
    "        start = max(start + chunk_size - overlap, end)\n",
    "        chunk_num += 1\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def _build_page_local_text(words: List[WordBox]) -> Tuple[str, List[Tuple[WordBox,int,int]]]:\n",
    "    \"\"\"Build local text from page words with offset mapping\"\"\"\n",
    "    mapping = []\n",
    "    cur = 0\n",
    "    sorted_words = sorted(words, key=lambda x: x.offset)\n",
    "    \n",
    "    for w in sorted_words:\n",
    "        token = w.text or ''\n",
    "        mapping.append((w, cur, len(token)))\n",
    "        cur += len(token) + 1\n",
    "    \n",
    "    page_text = ' '.join(w.text for w in sorted_words)\n",
    "    return page_text, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "158c1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ALLOWLIST FILTERING\n",
    "# =============================================================================\n",
    "\n",
    "def is_allowed_organization(entity_text: str) -> bool:\n",
    "    \"\"\"Check if entity text matches any allowed organization\"\"\"\n",
    "    entity_lower = entity_text.lower().strip()\n",
    "    \n",
    "    if entity_lower in ALLOWED_ORGANIZATIONS_LOWER:\n",
    "        return True\n",
    "    \n",
    "    for allowed_org in ALLOWED_ORGANIZATIONS_LOWER:\n",
    "        if allowed_org in entity_lower or entity_lower in allowed_org:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_allowed_person(entity_text: str) -> bool:\n",
    "    \"\"\"Check if entity text matches any allowed person/role\"\"\"\n",
    "    entity_lower = entity_text.lower().strip()\n",
    "    \n",
    "    if entity_lower in ALLOWED_PERSONS_LOWER:\n",
    "        return True\n",
    "    \n",
    "    for allowed_person in ALLOWED_PERSONS_LOWER:\n",
    "        if allowed_person in entity_lower or entity_lower in allowed_person:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_allowed_date(entity_category: str) -> bool:\n",
    "    \"\"\"Check if date entity should be allowed (all dates except DateOfBirth)\"\"\"\n",
    "    allowed_date_categories = [\n",
    "        'DateTime',\n",
    "        'Date',\n",
    "        'Time',\n",
    "        'DateRange',\n",
    "        'TimeRange'\n",
    "    ]\n",
    "    \n",
    "    if entity_category == 'DateOfBirth':\n",
    "        return False\n",
    "    \n",
    "    return entity_category in allowed_date_categories\n",
    "\n",
    "def should_redact_url(url_text: str) -> bool:\n",
    "    \"\"\"Check if URL should be redacted based on patterns\"\"\"\n",
    "    url_text = url_text.strip()\n",
    "    \n",
    "    for regex_pattern in REDACT_URL_REGEX:\n",
    "        if regex_pattern.match(url_text):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def filter_allowed_entities(entities: List[EntityPII], full_text: str = \"\") -> Tuple[List[EntityPII], List[EntityPII]]:\n",
    "    \"\"\"Filter entities, separating allowed from to-be-redacted\"\"\"\n",
    "    \n",
    "    to_redact = []\n",
    "    allowed = []\n",
    "    \n",
    "    for entity in entities:\n",
    "        should_allow = False\n",
    "        \n",
    "        if entity.category == 'Organization' and is_allowed_organization(entity.text):\n",
    "            should_allow = True\n",
    "        \n",
    "        elif entity.category in ['Person', 'PersonType'] and is_allowed_person(entity.text):\n",
    "            should_allow = True\n",
    "        \n",
    "        elif is_allowed_date(entity.category):\n",
    "            should_allow = True\n",
    "        \n",
    "        elif entity.category == 'DateOfBirth':\n",
    "            should_allow = False\n",
    "        \n",
    "        elif entity.category == 'URL' and entity.text.startswith(('http://', 'https://')):\n",
    "            if not should_redact_url(entity.text):\n",
    "                should_allow = True\n",
    "        \n",
    "        elif entity.category == 'URL' and not entity.text.startswith(('http://', 'https://')):\n",
    "            should_allow = True\n",
    "        \n",
    "        if should_allow:\n",
    "            allowed.append(entity)\n",
    "        else:\n",
    "            to_redact.append(entity)\n",
    "    \n",
    "    return to_redact, allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9feb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMAGE AND PDF PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def pdf_to_images(pdf_path: str, out_dir: str, dpi: int = 200) -> List[Path]:\n",
    "    \"\"\"Render each PDF page to PNG images\"\"\"\n",
    "    out = Path(out_dir)\n",
    "    out.mkdir(exist_ok=True)\n",
    "    pages = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for i, page in enumerate(doc):\n",
    "            mat = fitz.Matrix(dpi/72.0, dpi/72.0)\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "            img_path = out / f\"page_{i+1:04d}.png\"\n",
    "            pix.save(img_path.as_posix())\n",
    "            pages.append(img_path)\n",
    "    return pages\n",
    "\n",
    "def images_to_pdf(image_paths: List[Path], output_pdf: str):\n",
    "    \"\"\"Convert sequence of images back to PDF\"\"\"\n",
    "    doc = fitz.open()\n",
    "    for img_path in image_paths:\n",
    "        img = fitz.Pixmap(img_path.as_posix())\n",
    "        page = doc.new_page(width=img.width, height=img.height)\n",
    "        page.insert_image(page.rect, filename=img_path.as_posix())\n",
    "    doc.save(output_pdf, deflate=True)\n",
    "    doc.close()\n",
    "\n",
    "def _normalize_polygon(poly: List[float], img_w: int, img_h: int) -> List[float]:\n",
    "    \"\"\"Detect if coords are normalized (0-1) and scale to pixels\"\"\"\n",
    "    if not poly:\n",
    "        return poly\n",
    "    xs = poly[0::2]; ys = poly[1::2]\n",
    "    max_x = max(xs); max_y = max(ys)\n",
    "    if max_x <= 1.2 and max_y <= 1.2:\n",
    "        scaled = []\n",
    "        for i in range(0, len(poly), 2):\n",
    "            scaled.append(poly[i] * img_w)\n",
    "            scaled.append(poly[i+1] * img_h)\n",
    "        return scaled\n",
    "    return poly\n",
    "\n",
    "def _bbox_from_polygon(poly: List[float]):\n",
    "    \"\"\"Convert polygon to bounding box\"\"\"\n",
    "    if not poly:\n",
    "        return (0,0,0,0)\n",
    "    xs = poly[0::2]; ys = poly[1::2]\n",
    "    return (min(xs), min(ys), max(xs), max(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0ee2728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OCR WITH AZURE DOCUMENT INTELLIGENCE\n",
    "# =============================================================================\n",
    "\n",
    "def ocr_image_docint(image_path: Path, model_id: str = 'prebuilt-layout') -> Dict[str, Any]:\n",
    "    \"\"\"Perform OCR on single image using Document Intelligence\"\"\"\n",
    "    client = get_docint_client()\n",
    "    with open(image_path, 'rb') as f:\n",
    "        try:\n",
    "            poller = client.begin_analyze_document(model_id=model_id, body=f.read(), content_type='image/png', features=['barcodes'])\n",
    "        except TypeError:\n",
    "            f.seek(0)\n",
    "            poller = client.begin_analyze_document(model_id=model_id, body=f.read(), content_type='image/png')\n",
    "    \n",
    "    result = poller.result()\n",
    "    img = Image.open(image_path)\n",
    "    w, h = img.size\n",
    "    words = []\n",
    "    barcodes = []\n",
    "    content = getattr(result, 'content', '') or ''\n",
    "    pages = getattr(result, 'pages', None) or []\n",
    "    \n",
    "    for page_index, page in enumerate(pages):\n",
    "        page_words = getattr(page, 'words', None) or []\n",
    "        for wobj in page_words:\n",
    "            poly = _normalize_polygon(getattr(wobj, 'polygon', []) or [], w, h)\n",
    "            bbox = _bbox_from_polygon(poly)\n",
    "            span = getattr(wobj, 'span', None)\n",
    "            offset = getattr(span, 'offset', 0) if span else 0\n",
    "            length = getattr(span, 'length', 0) if span else len(getattr(wobj, 'content', ''))\n",
    "            \n",
    "            word_text = getattr(wobj, 'content', '')\n",
    "            words.append({\n",
    "                'text': word_text,\n",
    "                'offset': offset,\n",
    "                'length': length,\n",
    "                'bbox': bbox\n",
    "            })\n",
    "        \n",
    "        page_barcodes = getattr(page, 'barcodes', None) or []\n",
    "        if REDACT_BARCODES and page_barcodes:\n",
    "            for bc in page_barcodes:\n",
    "                poly_bc = _normalize_polygon(getattr(bc, 'polygon', []) or [], w, h)\n",
    "                bbox_bc = _bbox_from_polygon(poly_bc)\n",
    "                barcode_value = getattr(bc, 'value', '')\n",
    "                barcodes.append({\n",
    "                    'value': barcode_value,\n",
    "                    'kind': getattr(bc, 'kind', 'barcode'),\n",
    "                    'bbox': bbox_bc,\n",
    "                    'page': page_index\n",
    "                })\n",
    "    \n",
    "    return {'content': content, 'words': words, 'barcodes': barcodes, 'size': (w,h)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d54b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PII DETECTION WITH AZURE TEXT ANALYTICS\n",
    "# =============================================================================\n",
    "\n",
    "def _process_single_text(client, text: str, page_num: int, language: str) -> List[EntityPII]:\n",
    "    \"\"\"Process text with single API call\"\"\"\n",
    "    entities = []\n",
    "    \n",
    "    resp = client.recognize_pii_entities(\n",
    "        documents=[{\"id\": f\"page_{page_num}\", \"text\": text, \"language\": language}],\n",
    "        model_version=\"2025-08-01-preview\"\n",
    "    )\n",
    "    \n",
    "    for doc in resp:\n",
    "        if not doc.is_error:\n",
    "            for ent in doc.entities:\n",
    "                entities.append(EntityPII(\n",
    "                    category=str(ent.category),\n",
    "                    offset=ent.offset,\n",
    "                    length=ent.length,\n",
    "                    text=ent.text,\n",
    "                    page=page_num\n",
    "                ))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def _process_with_chunks(client, text: str, page_num: int, language: str, chunk_size: int) -> Tuple[List[EntityPII], Dict[str, Any]]:\n",
    "    \"\"\"Process text with chunking strategy\"\"\"\n",
    "    chunks = create_text_chunks(text, chunk_size)\n",
    "    chunk_entities = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            resp = client.recognize_pii_entities(\n",
    "                documents=[{\"id\": f\"page_{page_num}_{chunk.chunk_id}\", \"text\": chunk.text, \"language\": language}],\n",
    "                model_version=\"2025-08-01-preview\"\n",
    "            )\n",
    "            \n",
    "            for doc in resp:\n",
    "                if not doc.is_error:\n",
    "                    for ent in doc.entities:\n",
    "                        chunk_entities.append(ChunkEntity(\n",
    "                            category=str(ent.category),\n",
    "                            chunk_offset=ent.offset,\n",
    "                            length=ent.length,\n",
    "                            text=ent.text,\n",
    "                            global_offset=chunk.start_offset + ent.offset,\n",
    "                            chunk_id=chunk.chunk_id\n",
    "                        ))\n",
    "        except Exception as chunk_error:\n",
    "            if 'too large' in str(chunk_error).lower():\n",
    "                mini_chunks = create_text_chunks(chunk.text, TEXT_SIZE_THRESHOLDS['mini_chunk_size'])\n",
    "                for mini_chunk in mini_chunks:\n",
    "                    mini_chunk.start_offset += chunk.start_offset\n",
    "                    mini_chunk.end_offset += chunk.start_offset\n",
    "                    try:\n",
    "                        mini_resp = client.recognize_pii_entities(\n",
    "                            documents=[{\"id\": f\"page_{page_num}_{mini_chunk.chunk_id}\", \"text\": mini_chunk.text, \"language\": language}],\n",
    "                            model_version=\"2025-08-01-preview\"\n",
    "                        )\n",
    "                        for mini_doc in mini_resp:\n",
    "                            if not mini_doc.is_error:\n",
    "                                for ent in mini_doc.entities:\n",
    "                                    chunk_entities.append(ChunkEntity(\n",
    "                                        category=str(ent.category),\n",
    "                                        chunk_offset=ent.offset,\n",
    "                                        length=ent.length,\n",
    "                                        text=ent.text,\n",
    "                                        global_offset=mini_chunk.start_offset + ent.offset,\n",
    "                                        chunk_id=mini_chunk.chunk_id\n",
    "                                    ))\n",
    "                    except:\n",
    "                        pass\n",
    "    \n",
    "    global_entities = []\n",
    "    for chunk_ent in chunk_entities:\n",
    "        global_entities.append(EntityPII(\n",
    "            category=chunk_ent.category,\n",
    "            offset=chunk_ent.global_offset,\n",
    "            length=chunk_ent.length,\n",
    "            text=chunk_ent.text,\n",
    "            page=page_num\n",
    "        ))\n",
    "    \n",
    "    return global_entities, {\n",
    "        'chunks_created': len(chunks),\n",
    "        'chunks_processed': len(chunks),\n",
    "        'chunk_size_used': chunk_size\n",
    "    }\n",
    "\n",
    "def _process_with_emergency_chunks(client, text: str, page_num: int, language: str) -> Tuple[List[EntityPII], Dict[str, Any]]:\n",
    "    \"\"\"Emergency fallback chunking with smaller sizes\"\"\"\n",
    "    emergency_size = TEXT_SIZE_THRESHOLDS['emergency_chunk_size']\n",
    "    chunks = create_text_chunks(text, emergency_size, overlap=50)\n",
    "    \n",
    "    entities = []\n",
    "    successful_chunks = 0\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            resp = client.recognize_pii_entities(\n",
    "                documents=[{\"id\": f\"page_{page_num}_emergency_{chunk.chunk_id}\", \"text\": chunk.text, \"language\": language}],\n",
    "                model_version=\"2025-08-01-preview\"\n",
    "            )\n",
    "            \n",
    "            for doc in resp:\n",
    "                if not doc.is_error:\n",
    "                    for ent in doc.entities:\n",
    "                        entities.append(EntityPII(\n",
    "                            category=str(ent.category),\n",
    "                            offset=chunk.start_offset + ent.offset,\n",
    "                            length=ent.length,\n",
    "                            text=ent.text,\n",
    "                            page=page_num\n",
    "                        ))\n",
    "            successful_chunks += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return entities, {\n",
    "        'emergency_chunks_created': len(chunks),\n",
    "        'emergency_chunks_successful': successful_chunks,\n",
    "        'emergency_chunk_size': emergency_size\n",
    "    }\n",
    "\n",
    "def detect_pii_with_chunking(page_words: List[WordBox], page_num: int, language: str) -> Tuple[List[EntityPII], Dict[str, Any], List[EntityPII]]:\n",
    "    \"\"\"Enhanced PII detection with text size analysis, chunking, and allowlist filtering\"\"\"\n",
    "    if not page_words:\n",
    "        return [], {'strategy': 'no_text', 'length': 0}, []\n",
    "    \n",
    "    client = get_pii_client()\n",
    "    page_text = ' '.join(w.text or '' for w in sorted(page_words, key=lambda x: x.offset))\n",
    "    \n",
    "    if len(page_text) == 0:\n",
    "        return [], {'strategy': 'empty_text', 'length': 0}, []\n",
    "    \n",
    "    text_analysis = analyze_text_size(page_text)\n",
    "    \n",
    "    raw_entities = []\n",
    "    \n",
    "    try:\n",
    "        if text_analysis['strategy'] == 'single_call':\n",
    "            raw_entities = _process_single_text(client, page_text, page_num, language)\n",
    "            text_analysis['chunks_processed'] = 1\n",
    "            \n",
    "        elif text_analysis['strategy'] == 'standard_chunks':\n",
    "            raw_entities, chunk_info = _process_with_chunks(\n",
    "                client, page_text, page_num, language, \n",
    "                TEXT_SIZE_THRESHOLDS['chunk_size']\n",
    "            )\n",
    "            text_analysis.update(chunk_info)\n",
    "            \n",
    "        else:\n",
    "            raw_entities, chunk_info = _process_with_chunks(\n",
    "                client, page_text, page_num, language, \n",
    "                TEXT_SIZE_THRESHOLDS['mini_chunk_size']\n",
    "            )\n",
    "            text_analysis.update(chunk_info)\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e).lower()\n",
    "        if 'too large' in error_msg or 'request too long' in error_msg:\n",
    "            try:\n",
    "                raw_entities, chunk_info = _process_with_emergency_chunks(\n",
    "                    client, page_text, page_num, language\n",
    "                )\n",
    "                text_analysis.update(chunk_info)\n",
    "                text_analysis['emergency_fallback'] = True\n",
    "            except Exception as emergency_error:\n",
    "                text_analysis['final_error'] = str(emergency_error)\n",
    "        else:\n",
    "            text_analysis['error'] = str(e)\n",
    "    \n",
    "    entities_to_redact, allowed_entities = filter_allowed_entities(raw_entities, page_text)\n",
    "    \n",
    "    text_analysis['total_entities_found'] = len(raw_entities)\n",
    "    text_analysis['entities_to_redact'] = len(entities_to_redact)\n",
    "    text_analysis['entities_allowed'] = len(allowed_entities)\n",
    "    \n",
    "    return entities_to_redact, text_analysis, allowed_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3cdead8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REDACTION APPLICATION\n",
    "# =============================================================================\n",
    "\n",
    "def map_entities_to_redactions_simple(entities: List[EntityPII], page_words: List[WordBox], page_num: int) -> List[Redaction]:\n",
    "    \"\"\"Map PII entities to redaction rectangles\"\"\"\n",
    "    redactions = []\n",
    "    \n",
    "    if not entities or not page_words:\n",
    "        return redactions\n",
    "    \n",
    "    page_text, word_mapping = _build_page_local_text(page_words)\n",
    "    \n",
    "    for entity in entities:\n",
    "        entity_rects = []\n",
    "        \n",
    "        for word, word_start, word_len in word_mapping:\n",
    "            word_end = word_start + word_len\n",
    "            \n",
    "            entity_start = entity.offset\n",
    "            entity_end = entity.offset + entity.length\n",
    "            \n",
    "            if not (word_end <= entity_start or word_start >= entity_end):\n",
    "                entity_rects.append(word.bbox)\n",
    "        \n",
    "        if entity_rects:\n",
    "            merged_rects = merge_rects(entity_rects)\n",
    "            for rect in merged_rects:\n",
    "                redactions.append(Redaction(\n",
    "                    page=page_num,\n",
    "                    rect=rect,\n",
    "                    label=f\"{entity.category}:{entity.text[:20]}\"\n",
    "                ))\n",
    "        else:\n",
    "            entity_text_lower = entity.text.lower()\n",
    "            for word in page_words:\n",
    "                if word.text and entity_text_lower in word.text.lower():\n",
    "                    redactions.append(Redaction(\n",
    "                        page=page_num,\n",
    "                        rect=word.bbox,\n",
    "                        label=f\"{entity.category}:{entity.text[:20]}\"\n",
    "                    ))\n",
    "    \n",
    "    return redactions\n",
    "\n",
    "def apply_redactions_simple(image_path: Path, redactions: List[Redaction], output_path: Path):\n",
    "    \"\"\"Apply redactions to image by drawing black rectangles\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        for redaction in redactions:\n",
    "            x1, y1, x2, y2 = redaction.rect\n",
    "            draw.rectangle([x1, y1, x2, y2], fill='black')\n",
    "        \n",
    "        img.save(output_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        import shutil\n",
    "        shutil.copy2(image_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f271a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PAGE PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def process_single_page_simple(\n",
    "    image_path: Path,\n",
    "    page_num: int,\n",
    "    output_dir: Path,\n",
    "    language: str = 'pt-BR'\n",
    ") -> PageResult:\n",
    "    \"\"\"Process a single page with OCR, PII detection, and redaction\"\"\"\n",
    "    try:\n",
    "        ocr_result = ocr_image_docint(image_path)\n",
    "        \n",
    "        page_words = []\n",
    "        for w in ocr_result['words']:\n",
    "            page_words.append(WordBox(\n",
    "                page=0,\n",
    "                text=w['text'],\n",
    "                offset=w['offset'],\n",
    "                length=w['length'],\n",
    "                bbox=w['bbox']\n",
    "            ))\n",
    "        \n",
    "        page_entities, text_analysis, allowed_entities = detect_pii_with_chunking(page_words, page_num, language)\n",
    "        \n",
    "        page_redactions = map_entities_to_redactions_simple(page_entities, page_words, page_num)\n",
    "        \n",
    "        page_barcodes = []\n",
    "        for bc in ocr_result.get('barcodes', []):\n",
    "            bc_copy = bc.copy()\n",
    "            bc_copy['page'] = page_num\n",
    "            page_barcodes.append(bc_copy)\n",
    "        \n",
    "        if page_barcodes and REDACT_BARCODES:\n",
    "            for bc in page_barcodes:\n",
    "                bbox = bc.get('bbox', (0,0,0,0))\n",
    "                page_redactions.append(Redaction(\n",
    "                    page=page_num,\n",
    "                    rect=bbox,\n",
    "                    label=f\"BARCODE:{bc.get('kind', 'code')}\"\n",
    "                ))\n",
    "        \n",
    "        redacted_path = output_dir / f\"redacted_page_{page_num+1:04d}.png\"\n",
    "        apply_redactions_simple(image_path, page_redactions, redacted_path)\n",
    "        \n",
    "        result = PageResult(\n",
    "            page_num=page_num,\n",
    "            entities=page_entities,\n",
    "            redactions=page_redactions,\n",
    "            barcodes=page_barcodes,\n",
    "            success=True,\n",
    "            word_count=len(page_words),\n",
    "            redacted_image_path=redacted_path,\n",
    "            text_analysis=text_analysis,\n",
    "            allowed_entities=allowed_entities\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return PageResult(\n",
    "            page_num=page_num,\n",
    "            entities=[],\n",
    "            redactions=[],\n",
    "            barcodes=[],\n",
    "            success=False,\n",
    "            error_message=str(e),\n",
    "            word_count=0,\n",
    "            text_analysis={'error': str(e)},\n",
    "            allowed_entities=[]\n",
    "        )\n",
    "\n",
    "def process_pages_sequential_simple(images: List[Path], output_dir: Path, language: str = 'pt-BR') -> List[PageResult]:\n",
    "    \"\"\"Process pages sequentially one by one\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, image_path in enumerate(images):\n",
    "        result = process_single_page_simple(image_path, i, output_dir, language)\n",
    "        results.append(result)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "925bfeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN ANONYMIZATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def simple_anonymization_pipeline(pdf_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Enhanced PDF anonymization pipeline with text size analysis and allowlist protection\"\"\"\n",
    "    \n",
    "    pdf_path = str(Path(DATA_PATH) / pdf_name)\n",
    "    \n",
    "    import tempfile\n",
    "    work_dir = Path(tempfile.mkdtemp(prefix='pdf_processing_'))\n",
    "    \n",
    "    images = pdf_to_images(pdf_path, out_dir=str(work_dir / 'original'))\n",
    "    \n",
    "    output_dir = work_dir / 'redacted'\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    page_results = process_pages_sequential_simple(images, output_dir)\n",
    "    \n",
    "    redacted_images = []\n",
    "    for result in sorted(page_results, key=lambda r: r.page_num):\n",
    "        if result.success and result.redacted_image_path and result.redacted_image_path.exists():\n",
    "            redacted_images.append(result.redacted_image_path)\n",
    "    \n",
    "    if redacted_images:\n",
    "        final_pdf_path = str(Path(OUT_PATH) / f\"protected_anon_{pdf_name}\")\n",
    "        images_to_pdf(redacted_images, final_pdf_path)\n",
    "        success = True\n",
    "    else:\n",
    "        final_pdf_path = None\n",
    "        success = False\n",
    "    \n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(work_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    successful_results = [r for r in page_results if r.success]\n",
    "    total_entities = sum(len(r.entities) for r in successful_results)\n",
    "    total_redactions = sum(len(r.redactions) for r in successful_results)\n",
    "    total_allowed = sum(len(r.allowed_entities or []) for r in successful_results)\n",
    "    \n",
    "    entities_by_category = {}\n",
    "    for result in successful_results:\n",
    "        for entity in result.entities:\n",
    "            entities_by_category.setdefault(entity.category, 0)\n",
    "            entities_by_category[entity.category] += 1\n",
    "    \n",
    "    allowed_by_category = {}\n",
    "    for result in successful_results:\n",
    "        if result.allowed_entities:\n",
    "            for entity in result.allowed_entities:\n",
    "                allowed_by_category.setdefault(entity.category, 0)\n",
    "                allowed_by_category[entity.category] += 1\n",
    "    \n",
    "    text_stats = {\n",
    "        'single_call_pages': 0,\n",
    "        'chunked_pages': 0,\n",
    "        'emergency_pages': 0,\n",
    "        'total_chunks_processed': 0\n",
    "    }\n",
    "    \n",
    "    for result in successful_results:\n",
    "        if result.text_analysis:\n",
    "            strategy = result.text_analysis.get('strategy', 'unknown')\n",
    "            if strategy == 'single_call':\n",
    "                text_stats['single_call_pages'] += 1\n",
    "            elif strategy in ['standard_chunks', 'mini_chunks']:\n",
    "                text_stats['chunked_pages'] += 1\n",
    "                text_stats['total_chunks_processed'] += result.text_analysis.get('chunks_processed', 0)\n",
    "            \n",
    "            if result.text_analysis.get('emergency_fallback'):\n",
    "                text_stats['emergency_pages'] += 1\n",
    "    \n",
    "    return {\n",
    "        'pdf_name': pdf_name,\n",
    "        'success': success,\n",
    "        'total_pages': len(images),\n",
    "        'successful_pages': len(successful_results),\n",
    "        'total_entities': total_entities,\n",
    "        'total_redactions': total_redactions,\n",
    "        'total_allowed_entities': total_allowed,\n",
    "        'entities_by_category': entities_by_category,\n",
    "        'allowed_entities_by_category': allowed_by_category,\n",
    "        'text_processing_stats': text_stats,\n",
    "        'output_pdf': final_pdf_path\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb1541a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° To run: run_simple_anonymization()\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_simple_anonymization():\n",
    "    \"\"\"Run the enhanced anonymization pipeline with expanded allowlist protection\"\"\"\n",
    "    \n",
    "    PDF_FILE = 'contoso_documento_profissional_sem_cpf.pdf'\n",
    "    \n",
    "    if EXECUTION_CONTROL.get('pipeline_executed', False):\n",
    "        print(\"‚ö†Ô∏è Pipeline already executed!\")\n",
    "        print(\"üí° To run again: set EXECUTION_CONTROL['pipeline_executed'] = False\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üöÄ Starting PDF Anonymization Pipeline...\")\n",
    "    \n",
    "    try:\n",
    "        EXECUTION_CONTROL['pipeline_executed'] = True\n",
    "        \n",
    "        results = simple_anonymization_pipeline(PDF_FILE)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üìä RESULTS SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"üìÑ Document: {results['pdf_name']}\")\n",
    "        print(f\"üìë Pages: {results['successful_pages']}/{results['total_pages']} successful\")\n",
    "        print(f\"üîç PII entities found: {results['total_entities']} (to redact)\")\n",
    "        print(f\"üõ°Ô∏è Protected entities: {results['total_allowed_entities']} (NOT redacted)\")\n",
    "        print(f\"üéØ Redactions applied: {results['total_redactions']}\")\n",
    "        \n",
    "        text_stats = results.get('text_processing_stats', {})\n",
    "        print(f\"\\nüìä TEXT PROCESSING:\")\n",
    "        print(f\"   Single API call: {text_stats.get('single_call_pages', 0)} pages\")\n",
    "        print(f\"   Chunked processing: {text_stats.get('chunked_pages', 0)} pages\")\n",
    "        print(f\"   Emergency fallback: {text_stats.get('emergency_pages', 0)} pages\")\n",
    "        print(f\"   Total chunks: {text_stats.get('total_chunks_processed', 0)}\")\n",
    "        \n",
    "        if results['entities_by_category']:\n",
    "            print(f\"\\nüè∑Ô∏è PII CATEGORIES (TO REDACT):\")\n",
    "            for category, count in sorted(results['entities_by_category'].items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"   - {category}: {count}\")\n",
    "        \n",
    "        if results['allowed_entities_by_category']:\n",
    "            print(f\"\\nüõ°Ô∏è PROTECTED CATEGORIES (NOT REDACTED):\")\n",
    "            for category, count in sorted(results['allowed_entities_by_category'].items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"   - {category}: {count}\")\n",
    "        \n",
    "        if results['success']:\n",
    "            print(f\"\\n‚úÖ SUCCESS!\")\n",
    "            print(f\"üìÅ Output: {results['output_pdf']}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå FAILED!\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        EXECUTION_CONTROL['pipeline_executed'] = False\n",
    "        print(f\"‚ùå Error: PDF file '{PDF_FILE}' not found in '{DATA_PATH}' directory\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        EXECUTION_CONTROL['pipeline_executed'] = False\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"üí° To run: run_simple_anonymization()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b26839c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting PDF Anonymization Pipeline...\n",
      "\n",
      "======================================================================\n",
      "üìä RESULTS SUMMARY\n",
      "======================================================================\n",
      "üìÑ Document: contoso_documento_profissional_sem_cpf.pdf\n",
      "üìë Pages: 1/1 successful\n",
      "üîç PII entities found: 4 (to redact)\n",
      "üõ°Ô∏è Protected entities: 2 (NOT redacted)\n",
      "üéØ Redactions applied: 7\n",
      "\n",
      "üìä TEXT PROCESSING:\n",
      "   Single API call: 1 pages\n",
      "   Chunked processing: 0 pages\n",
      "   Emergency fallback: 0 pages\n",
      "   Total chunks: 0\n",
      "\n",
      "üè∑Ô∏è PII CATEGORIES (TO REDACT):\n",
      "   - Person: 1\n",
      "   - Email: 1\n",
      "   - PhoneNumber: 1\n",
      "   - URL: 1\n",
      "\n",
      "üõ°Ô∏è PROTECTED CATEGORIES (NOT REDACTED):\n",
      "   - Organization: 1\n",
      "   - URL: 1\n",
      "\n",
      "‚úÖ SUCCESS!\n",
      "üìÅ Output: anonimized\\protected_anon_contoso_documento_profissional_sem_cpf.pdf\n",
      "\n",
      "======================================================================\n",
      "üìä RESULTS SUMMARY\n",
      "======================================================================\n",
      "üìÑ Document: contoso_documento_profissional_sem_cpf.pdf\n",
      "üìë Pages: 1/1 successful\n",
      "üîç PII entities found: 4 (to redact)\n",
      "üõ°Ô∏è Protected entities: 2 (NOT redacted)\n",
      "üéØ Redactions applied: 7\n",
      "\n",
      "üìä TEXT PROCESSING:\n",
      "   Single API call: 1 pages\n",
      "   Chunked processing: 0 pages\n",
      "   Emergency fallback: 0 pages\n",
      "   Total chunks: 0\n",
      "\n",
      "üè∑Ô∏è PII CATEGORIES (TO REDACT):\n",
      "   - Person: 1\n",
      "   - Email: 1\n",
      "   - PhoneNumber: 1\n",
      "   - URL: 1\n",
      "\n",
      "üõ°Ô∏è PROTECTED CATEGORIES (NOT REDACTED):\n",
      "   - Organization: 1\n",
      "   - URL: 1\n",
      "\n",
      "‚úÖ SUCCESS!\n",
      "üìÅ Output: anonimized\\protected_anon_contoso_documento_profissional_sem_cpf.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pdf_name': 'contoso_documento_profissional_sem_cpf.pdf',\n",
       " 'success': True,\n",
       " 'total_pages': 1,\n",
       " 'successful_pages': 1,\n",
       " 'total_entities': 4,\n",
       " 'total_redactions': 7,\n",
       " 'total_allowed_entities': 2,\n",
       " 'entities_by_category': {'Person': 1, 'Email': 1, 'PhoneNumber': 1, 'URL': 1},\n",
       " 'allowed_entities_by_category': {'Organization': 1, 'URL': 1},\n",
       " 'text_processing_stats': {'single_call_pages': 1,\n",
       "  'chunked_pages': 0,\n",
       "  'emergency_pages': 0,\n",
       "  'total_chunks_processed': 0},\n",
       " 'output_pdf': 'anonimized\\\\protected_anon_contoso_documento_profissional_sem_cpf.pdf'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset execution control\n",
    "EXECUTION_CONTROL['pipeline_executed'] = False\n",
    "\n",
    "run_simple_anonymization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
